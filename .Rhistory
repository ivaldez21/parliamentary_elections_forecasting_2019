seed = 1234,                  ##
training_frame = train,      ## the H2O frame for training
validation_frame = valid,    ## the H2O frame for validation (not required)
x = x,                       ## the predictor columns, by column index
y = y  #log_perc_win              ## the target index (what we are predicting) "log_perc_win"
)
var_imp_linear = h2o.varimp(xgb5)
var_imp_linear
h2o.performance(xgb5)
prediction_perc_for = h2o.predict(xgb5, newdata = df_test) %>% as.data.frame() #auto training the data
predict_df$prediction_perc_for = prediction_perc_for[,1]
# getting the winners based on who has the highest prediction in each tvo
predict_df %<>% group_by(tvo) %>% mutate(winner_prediction_by_perc = prediction_perc_for == max(prediction_perc_for))
names(predict_df)
######## write the file with both linear and binary predictors ###################################
write.csv(predict_df, file = "predictions_updated.csv", row.names = F)
######## NOT RUNNING THE SCRAPER AGAIN ############################
results_raw = read_csv("2019_results_raw.csv")
names(results_raw) = c("candidate", "graph", "perc_for",
"votes_for", "tvo", "candidate_url", "percent_counted")
##### Clean the raw extracted data #####################
results = results_raw #keeping a copy
sapply(results, mode)
results[['graph']] <- NULL
#fix the comma/deciml/space problem in numeric columns
for (col in c("perc_for", "votes_for", "percent_counted")) {
results[[col]] %<>%
gsub(pattern = ",", replacement = ".") %>%
gsub(pattern = "\\s", replacement = "") %>%
as.numeric()
}
sapply(results, class)
#split the candidate column into candidate and proposed party
results[["proposed"]] = results[["candidate"]] %>%
strsplit(split = ",") %>% sapply(FUN = `[[`, 2) %>% trimws()
results[["candidate"]] %<>%
strsplit(split = ",") %>% sapply(FUN = `[[`, 1) %>% trimws()
# results %<>% mutate_if(is.character,as.factor)
sapply(results, class)
#shorten the party names for graphs
for (i in 1:nrow(results)) {
results$proposed[i] = switch(results$proposed[i],
'ПОЛІТИЧНА ПАРТІЯ "СЛУГА НАРОДУ"' = "Слуга Народу",
"самовисування" = "самовисування",
'Політична партія "ОПОЗИЦІЙНА ПЛАТФОРМА – ЗА ЖИТТЯ"' = "Опозиційна платформа",
'Політична партія "ОПОЗИЦІЙНИЙ БЛОК"' = "Опозиційний блок",
'політична партія Всеукраїнське об’єднання "Батьківщина"' = "Батьківщина",
'Політична Партія "ГОЛОС"' = 'Голос',
'Політична партія "Європейська Солідарність"' = "Європейська Солідарність",
"Other")
results$candidate[i] = gsub(pattern = "[[:punct:]]", replacement = "", x = results$candidate[i])
}
#group by tvo and take only the top one people
winners_df = results %>% group_by(tvo) %>% top_n(n = 2, wt = perc_for) %>%
mutate(spread = perc_for - min(perc_for)) %>% top_n(n = 1, wt = perc_for)
# Plot for number of winners per party
# Fitting Labels
par(las=2) # make label text perpendicular to axis
par(mar=c(5,12,4,2)) # increase y-axis margin.
par(mar=c(2,2,2,2)) # increase y-axis margin.
# compare winners to predictions
predictions_df = read_csv("predictions.csv")
clean_predictions_df = read_csv("combined_results_cleaned.csv")
predictions_df_top = read_csv("top_2_per_tvo.csv") #original predictions file
predictions_updated = read_csv("predictions_updated.csv")
# this is taking the top using perc_for linear method
predictions_updated %<>% group_by(tvo) %>% top_n(1, wt = prediction_150)
predictions_updated = predictions_updated[-which(duplicated(predictions_updated$tvo)),]
# set up the confusion matrix. Can change predicted to see performance of different models
comparison_df = data.frame(
predicted = predictions_updated$deputat, #predictions_updated$deputat,
atual = winners_df$candidate,
tvo = winners_df$tvo,
party = winners_df$proposed,
spread = winners_df$spread,
stringsAsFactors = F)
correct = comparison_df$predicted == comparison_df$atual
table(correct)
predictions_updated = read_csv("predictions_updated.csv")
# this is taking the top using perc_for linear method
predictions_updated %<>% group_by(tvo) %>% top_n(1, wt = prediction_100)
predictions_updated = predictions_updated[-which(duplicated(predictions_updated$tvo)),]
# set up the confusion matrix. Can change predicted to see performance of different models
comparison_df = data.frame(
predicted = predictions_updated$deputat, #predictions_updated$deputat,
atual = winners_df$candidate,
tvo = winners_df$tvo,
party = winners_df$proposed,
spread = winners_df$spread,
stringsAsFactors = F)
correct = comparison_df$predicted == comparison_df$atual
table(correct)
predictions_updated = read_csv("predictions_updated.csv")
# this is taking the top using perc_for linear method
predictions_updated %<>% group_by(tvo) %>% top_n(1, wt = prediction_50)
predictions_updated = predictions_updated[-which(duplicated(predictions_updated$tvo)),]
# set up the confusion matrix. Can change predicted to see performance of different models
comparison_df = data.frame(
predicted = predictions_updated$deputat, #predictions_updated$deputat,
atual = winners_df$candidate,
tvo = winners_df$tvo,
party = winners_df$proposed,
spread = winners_df$spread,
stringsAsFactors = F)
correct = comparison_df$predicted == comparison_df$atual
table(correct)
predictions_updated = read_csv("predictions_updated.csv")
# this is taking the top using perc_for linear method
predictions_updated %<>% group_by(tvo) %>% top_n(1, wt = prediction_50)
predictions_updated = predictions_updated[-which(duplicated(predictions_updated$tvo)),]
predictions_updated = read_csv("predictions_updated.csv")
# this is taking the top using perc_for linear method
predictions_updated %<>% group_by(tvo) %>% top_n(1, wt = prediction_50)
predictions_updated = predictions_updated[-which(duplicated(predictions_updated$tvo)),]
predictions_updated = read_csv("predictions_updated.csv")
# this is taking the top using perc_for linear method
predictions_updated %<>% group_by(tvo) %>% top_n(1, wt = prediction_50)
# set up the confusion matrix. Can change predicted to see performance of different models
comparison_df = data.frame(
predicted = predictions_updated$deputat, #predictions_updated$deputat,
atual = winners_df$candidate,
tvo = winners_df$tvo,
party = winners_df$proposed,
spread = winners_df$spread,
stringsAsFactors = F)
correct = comparison_df$predicted == comparison_df$atual
table(correct)
predictions_updated = read_csv("predictions_updated.csv")
# this is taking the top using perc_for linear method
predictions_updated %<>% group_by(tvo) %>% top_n(1, wt = prediction_100)
# set up the confusion matrix. Can change predicted to see performance of different models
comparison_df = data.frame(
predicted = predictions_updated$deputat, #predictions_updated$deputat,
atual = winners_df$candidate,
tvo = winners_df$tvo,
party = winners_df$proposed,
spread = winners_df$spread,
stringsAsFactors = F)
correct = comparison_df$predicted == comparison_df$atual
table(correct)
predictions_updated = read_csv("predictions_updated.csv")
# this is taking the top using perc_for linear method
predictions_updated %<>% group_by(tvo) %>% top_n(1, wt = prediction_150)
predictions_updated = predictions_updated[-which(duplicated(predictions_updated$tvo)),]
# set up the confusion matrix. Can change predicted to see performance of different models
comparison_df = data.frame(
predicted = predictions_updated$deputat, #predictions_updated$deputat,
atual = winners_df$candidate,
tvo = winners_df$tvo,
party = winners_df$proposed,
spread = winners_df$spread,
stringsAsFactors = F)
correct = comparison_df$predicted == comparison_df$atual
table(correct)
predictions_updated = read_csv("predictions_updated.csv")
# this is taking the top using perc_for linear method
predictions_updated %<>% group_by(tvo) %>% top_n(1, wt = prediction_perc_for)
predictions_updated = predictions_updated[-which(duplicated(predictions_updated$tvo)),]
predictions_updated = read_csv("predictions_updated.csv")
# this is taking the top using perc_for linear method
predictions_updated %<>% group_by(tvo) %>% top_n(1, wt = prediction_perc_for)
# set up the confusion matrix. Can change predicted to see performance of different models
comparison_df = data.frame(
predicted = predictions_updated$deputat, #predictions_updated$deputat,
atual = winners_df$candidate,
tvo = winners_df$tvo,
party = winners_df$proposed,
spread = winners_df$spread,
stringsAsFactors = F)
correct = comparison_df$predicted == comparison_df$atual
table(correct)
##### PACKAGES #######################
library(h2o)
library(readr)
library(data.table)
library(magrittr)
library(dplyr)
library(stringr)
Sys.setlocale(locale = "UTF-8")
setwd(dir = "/Users/isaiahlawrencevaldez/Documents/GitHub/parliamentary_elections_forecasting_2019/isaiah")
data = read_csv(file = "combined_results_cleaned.csv") %>% data.table(stringsAsFactors = T)
data[, tvo := as.factor(as.character(tvo))]
data[, tvo_pres := as.factor(as.character(tvo_pres))]
data[, log_perc_for := log1p(perc_for)]
# need this to split the data later on
data$unique_id = 1:nrow(data)
which(!unique(data$unique_id))
## H2O is an R package
library(h2o)
library(data.table)
## Create an H2O cloud
h2o.init(
nthreads=-1,            ## -1: use all available threads
max_mem_size = "10G")    ## specify the memory size for the H2O cloud
h2o.removeAll() # Clean slate - just in case the cluster was already running
## Load a file from disk
# df <- h2o.importFile(path = normalizePath("train_df.csv"))
df = data[which(data$year != 2019),]
train_df = df
df = as.h2o(df)
df_test = data[which(data$year == 2019),]
predict_df = df_test
df_test = as.h2o(df_test)
names(df)
h2o.isfactor(df)
## First, we will create three splits for train/test/valid independent data sets.
## We will train a data set on one set and use the others to test the validity
##  of model by ensuring that it can predict accurately on data the model has not
##  been shown.
## The second set will be used for validation most of the time. The third set will
##  be withheld until the end, to ensure that our validation accuracy is consistent
##  with data we have never seen during the iterative process.
splits <- h2o.splitFrame(
df,           ##  splitting the H2O frame we read above
c(0.6,0.2),   ##  create splits of 60% and 20%;
##  H2O will create one more split of 1-(sum of these parameters)
##  so we will get 0.6 / 0.2 / 1 - (0.6+0.2) = 0.6/0.2/0.2
seed=1234)    ##  setting a seed will ensure reproducible results (not R's seed)
train <- h2o.assign(splits[[1]], "train.hex")
valid <- h2o.assign(splits[[2]], "valid.hex")   ## R valid, H2O valid.hex
test <- h2o.assign(splits[[3]], "test.hex")     ## R test, H2O test.hex
rm(splits)
rm(data)
## take a look at the first few rows of the data set
train[1:5,]   ## rows 1-5, all columns
names(train)
feature_names = c("tvo", "oblast", "year", "proposed", "gender", "age", "party",
"average_age", "pop_change", "previous", "ever", "job", "funding",
"region_ideology", "power_status", "party_ideology", "prop_election_forecast")
feature_names
h2o.isfactor(train[,feature_names])
##### XGBOOST ####################################
x = feature_names
y = 20
names(train[,x])
names(train[,y])
xgb1 <- h2o.xgboost(            ## h2o.randomForest function
model_id = "xgb1",
ntrees = 150,
eta = 0.1,
min_child_weight = 9,        ## very imbalanced
max_depth = 11,
gamma = 0,                   ## should be tuned, depends on logloss function
max_delta_step = 0,          ## defaults to 0, can be helpful in unbalanced models
subsample = 0.8,             ## defaults to 1, lowering prevents overfit
colsample_bytree = 0.8,      ## fraction of columns to samples for each tree
stopping_rounds = 50,
stopping_metric = "logloss",
verbose = T,
seed = 1234,                  ##
training_frame = train,      ## the H2O frame for training
validation_frame = valid,    ## the H2O frame for validation (not required)
x = x,                       ## the predictor columns, by column index
y = y  #winner              ## the target index (what we are predicting) "log_perc_win"
)
xgb2 <- h2o.xgboost(            ## h2o.randomForest function
model_id = "xgb2",
ntrees = 100,
eta = 0.1,
min_child_weight = 9,        ## very imbalanced
max_depth = 11,
gamma = 0,                   ## should be tuned, depends on logloss function
max_delta_step = 0,          ## defaults to 0, can be helpful in unbalanced models
subsample = 0.8,             ## defaults to 1, lowering prevents overfit
colsample_bytree = 0.8,      ## fraction of columns to samples for each tree
stopping_rounds = 50,
stopping_metric = "logloss",
verbose = T,
seed = 1234,                  ##
training_frame = train,      ## the H2O frame for training
validation_frame = valid,    ## the H2O frame for validation (not required)
x = x,                       ## the predictor columns, by column index
y = y  #winner              ## the target index (what we are predicting) "log_perc_win"
)
xgb3 <- h2o.xgboost(            ## h2o.randomForest function
model_id = "xgb3",
ntrees = 50,
eta = 0.1,
min_child_weight = 9,        ## very imbalanced
max_depth = 11,
gamma = 0,                   ## should be tuned, depends on logloss function
max_delta_step = 0,          ## defaults to 0, can be helpful in unbalanced models
subsample = 0.8,             ## defaults to 1, lowering prevents overfit
colsample_bytree = 0.8,      ## fraction of columns to samples for each tree
stopping_rounds = 50,
stopping_metric = "logloss",
verbose = T,
seed = 1234,                  ##
training_frame = train,      ## the H2O frame for training
validation_frame = valid,    ## the H2O frame for validation (not required)
x = x,                       ## the predictor columns, by column index
y = y  #winner              ## the target index (what we are predicting) "log_perc_win"
)
xgb4 <- h2o.xgboost(            ## h2o.randomForest function
model_id = "xgb4",
min_child_weight = 9,
verbose = T,
seed = 1234,                  ##
training_frame = train,      ## the H2O frame for training
validation_frame = valid,    ## the H2O frame for validation (not required)
x = x,                       ## the predictor columns, by column index
y = y  #winner              ## the target index (what we are predicting) "log_perc_win"
)
var_imp = h2o.varimp(xgb1)
h2o.varimp(xgb1)
h2o.varimp(xgb2)
h2o.varimp(xgb3)
h2o.varimp(xgb4)
prediction_150 = h2o.predict(xgb1, newdata = df_test) %>% as.data.frame()
prediction_100 = h2o.predict(xgb2, newdata = df_test) %>% as.data.frame()
prediction_50 = h2o.predict(xgb3, newdata = df_test) %>% as.data.frame()
prediction_default_plus_class = h2o.predict(xgb4, newdata = df_test) %>% as.data.frame()
predict_df$prediction_150 = prediction_150[,3]
predict_df$prediction_100 = prediction_100[,3]
predict_df$prediction_50 = prediction_50[,3]
predict_df$prediction_default_plus_class = prediction_default_plus_class[,3]
pick_winners = function(df, tvo_column, vote_column) {
won = vector()
for (i in 1:nrow(df)) {
subset = df[which(df[[tvo_column]] == df[[tvo_column]][i]),]
max_in_tvo = max(subset[[vote_column]])
won[i] = ifelse(df[[vote_column]][i] == max_in_tvo, 1, 0)
}
won
}
predict_df$winner = pick_winners(predict_df, tvo_column = "tvo", vote_column = "prediction_150")
# temp_df = predict_df %>% group_by(proposed) %>% summarize(winner_count = sum(winner))
predict_df$tvo %<>% as.character() %>% as.numeric()
predict_df %<>% arrange(tvo,desc(prediction_150))
#### XGBOOST PREDICTING LOG_PERC_FOR (LINEAR MODEL) #######################################
x = feature_names
y = 30 #perc_for column
names(train[,x])
names(train[,y])
xgb5 <- h2o.xgboost(            ## h2o.randomForest function
model_id = "xgb5",
ntrees = 50,
eta = 0.1,
min_child_weight = 9,        ## very imbalanced
max_depth = 11,
gamma = 0,                   ## should be tuned, depends on logloss function
max_delta_step = 0,          ## defaults to 0, can be helpful in unbalanced models
subsample = 0.8,             ## defaults to 1, lowering prevents overfit
colsample_bytree = 0.8,      ## fraction of columns to samples for each tree
stopping_rounds = 50,
verbose = T,
seed = 1234,                  ##
training_frame = train,      ## the H2O frame for training
validation_frame = valid,    ## the H2O frame for validation (not required)
x = x,                       ## the predictor columns, by column index
y = y  #log_perc_win              ## the target index (what we are predicting) "log_perc_win"
)
xgb5 <- h2o.xgboost(            ## h2o.randomForest function
model_id = "xgb5",
ntrees = 50,
eta = 0.1,
min_child_weight = 9,        ## very imbalanced
max_depth = 11,
gamma = 0,                   ## should be tuned, depends on logloss function
max_delta_step = 0,          ## defaults to 0, can be helpful in unbalanced models
subsample = 0.8,             ## defaults to 1, lowering prevents overfit
colsample_bytree = 0.8,      ## fraction of columns to samples for each tree
stopping_rounds = 10,
verbose = T,
seed = 1234,                  ##
training_frame = train,      ## the H2O frame for training
validation_frame = valid,    ## the H2O frame for validation (not required)
x = x,                       ## the predictor columns, by column index
y = y  #log_perc_win              ## the target index (what we are predicting) "log_perc_win"
)
xgb5 <- h2o.xgboost(            ## h2o.randomForest function
model_id = "xgb5",
ntrees = 50,
eta = 0.1,
min_child_weight = 9,        ## very imbalanced
max_depth = 11,
gamma = 0,                   ## should be tuned, depends on logloss function
max_delta_step = 0,          ## defaults to 0, can be helpful in unbalanced models
subsample = 0.8,             ## defaults to 1, lowering prevents overfit
colsample_bytree = 0.8,      ## fraction of columns to samples for each tree
stopping_rounds = 5,
verbose = T,
seed = 1234,                  ##
training_frame = train,      ## the H2O frame for training
validation_frame = valid,    ## the H2O frame for validation (not required)
x = x,                       ## the predictor columns, by column index
y = y  #log_perc_win              ## the target index (what we are predicting) "log_perc_win"
)
var_imp_linear = h2o.varimp(xgb5)
var_imp_linear
h2o.performance(xgb5)
prediction_perc_for = h2o.predict(xgb5, newdata = df_test) %>% as.data.frame() #auto training the data
predict_df$prediction_perc_for = prediction_perc_for[,1]
# getting the winners based on who has the highest prediction in each tvo
predict_df %<>% group_by(tvo) %>% mutate(winner_prediction_by_perc = prediction_perc_for == max(prediction_perc_for))
names(predict_df)
######## write the file with both linear and binary predictors ###################################
write.csv(predict_df, file = "predictions_updated.csv", row.names = F)
######## NOT RUNNING THE SCRAPER AGAIN ############################
results_raw = read_csv("2019_results_raw.csv")
names(results_raw) = c("candidate", "graph", "perc_for",
"votes_for", "tvo", "candidate_url", "percent_counted")
##### Clean the raw extracted data #####################
results = results_raw #keeping a copy
sapply(results, mode)
results[['graph']] <- NULL
#fix the comma/deciml/space problem in numeric columns
for (col in c("perc_for", "votes_for", "percent_counted")) {
results[[col]] %<>%
gsub(pattern = ",", replacement = ".") %>%
gsub(pattern = "\\s", replacement = "") %>%
as.numeric()
}
sapply(results, class)
#split the candidate column into candidate and proposed party
results[["proposed"]] = results[["candidate"]] %>%
strsplit(split = ",") %>% sapply(FUN = `[[`, 2) %>% trimws()
results[["candidate"]] %<>%
strsplit(split = ",") %>% sapply(FUN = `[[`, 1) %>% trimws()
# results %<>% mutate_if(is.character,as.factor)
sapply(results, class)
#shorten the party names for graphs
for (i in 1:nrow(results)) {
results$proposed[i] = switch(results$proposed[i],
'ПОЛІТИЧНА ПАРТІЯ "СЛУГА НАРОДУ"' = "Слуга Народу",
"самовисування" = "самовисування",
'Політична партія "ОПОЗИЦІЙНА ПЛАТФОРМА – ЗА ЖИТТЯ"' = "Опозиційна платформа",
'Політична партія "ОПОЗИЦІЙНИЙ БЛОК"' = "Опозиційний блок",
'політична партія Всеукраїнське об’єднання "Батьківщина"' = "Батьківщина",
'Політична Партія "ГОЛОС"' = 'Голос',
'Політична партія "Європейська Солідарність"' = "Європейська Солідарність",
"Other")
results$candidate[i] = gsub(pattern = "[[:punct:]]", replacement = "", x = results$candidate[i])
}
#group by tvo and take only the top one people
winners_df = results %>% group_by(tvo) %>% top_n(n = 2, wt = perc_for) %>%
mutate(spread = perc_for - min(perc_for)) %>% top_n(n = 1, wt = perc_for)
# Plot for number of winners per party
# Fitting Labels
par(las=2) # make label text perpendicular to axis
par(mar=c(5,12,4,2)) # increase y-axis margin.
par(mar=c(2,2,2,2)) # increase y-axis margin.
# compare winners to predictions
predictions_df = read_csv("predictions.csv")
clean_predictions_df = read_csv("combined_results_cleaned.csv")
predictions_df_top = read_csv("top_2_per_tvo.csv") #original predictions file
predictions_updated = read_csv("predictions_updated.csv")
# this is taking the top using perc_for linear method
predictions_updated %<>% group_by(tvo) %>% top_n(1, wt = prediction_perc_for)
# set up the confusion matrix. Can change predicted to see performance of different models
comparison_df = data.frame(
predicted = predictions_updated$deputat, #predictions_updated$deputat,
atual = winners_df$candidate,
tvo = winners_df$tvo,
party = winners_df$proposed,
spread = winners_df$spread,
stringsAsFactors = F)
correct = comparison_df$predicted == comparison_df$atual
table(correct)
# this is taking the top using perc_for linear method
predictions_updated %<>% group_by(tvo) %>% top_n(1, wt = prediction_150)
predictions_updated = read_csv("predictions_updated.csv")
# this is taking the top using perc_for linear method
predictions_updated %<>% group_by(tvo) %>% top_n(1, wt = prediction_150)
# set up the confusion matrix. Can change predicted to see performance of different models
comparison_df = data.frame(
predicted = predictions_updated$deputat, #predictions_updated$deputat,
atual = winners_df$candidate,
tvo = winners_df$tvo,
party = winners_df$proposed,
spread = winners_df$spread,
stringsAsFactors = F)
correct = comparison_df$predicted == comparison_df$atual
table(correct)
predictions_updated = read_csv("predictions_updated.csv")
# this is taking the top using perc_for linear method
predictions_updated %<>% group_by(tvo) %>% top_n(1, wt = prediction_100)
# set up the confusion matrix. Can change predicted to see performance of different models
comparison_df = data.frame(
predicted = predictions_updated$deputat, #predictions_updated$deputat,
atual = winners_df$candidate,
tvo = winners_df$tvo,
party = winners_df$proposed,
spread = winners_df$spread,
stringsAsFactors = F)
correct = comparison_df$predicted == comparison_df$atual
table(correct)
predictions_updated = read_csv("predictions_updated.csv")
# this is taking the top using perc_for linear method
predictions_updated %<>% group_by(tvo) %>% top_n(1, wt = prediction_50)
# set up the confusion matrix. Can change predicted to see performance of different models
comparison_df = data.frame(
predicted = predictions_updated$deputat, #predictions_updated$deputat,
atual = winners_df$candidate,
tvo = winners_df$tvo,
party = winners_df$proposed,
spread = winners_df$spread,
stringsAsFactors = F)
correct = comparison_df$predicted == comparison_df$atual
table(correct)
names(winners_df)
combined_df_1 = left_join(winners_df, predictions_df, by = c("candidate" = "deputat", "tvo"))
combined_df = left_join(combined_df_1, predictions_df_top[,c("deputat", "tvo", "winner")], by = c("candidate" = "deputat", "tvo"))
names(combined_df)
table(combined_df$ever)
table(combined_df$previous)
head(combined_df$info)
table(combined_df$previous)
table(combined_df$job)
View(combined_df$candidate[which(combined_df$previous == 1),])
View(combined_df$candidate[which(combined_df$previous == 1)])
View(combined_df[which(combined_df$previous == 1), c("candidate", "tvo")])
